{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "def readMRIVolume(mri_volume_path):\n",
    "    reader = sitk.ImageSeriesReader()\n",
    "    dicom_files = reader.GetGDCMSeriesFileNames(mri_volume_path)\n",
    "    #'dicom_files' are the individual slices of an MRI volume\n",
    "    reader.SetFileNames(dicom_files)\n",
    "    retrieved_mri_volume = reader.Execute()\n",
    "    return retrieved_mri_volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  # Resample images to uniform voxel spacing\n",
    "import numpy as np  \n",
    "def resample_image(input_volume, out_spacing=[1, 1, 1]):\n",
    "  \n",
    "    original_spacing = input_volume.GetSpacing()\n",
    "    original_size = input_volume.GetSize()\n",
    "\n",
    "    out_size = [\n",
    "        int(np.round(original_size[0] * (original_spacing[0] / out_spacing[0]))),\n",
    "        int(np.round(original_size[1] * (original_spacing[1] / out_spacing[1]))),\n",
    "        int(np.round(original_size[2] * (original_spacing[2] / out_spacing[2])))]\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(out_spacing)\n",
    "    resample.SetSize(out_size)\n",
    "    resample.SetOutputDirection(input_volume.GetDirection())\n",
    "    resample.SetOutputOrigin(input_volume.GetOrigin())\n",
    "    resample.SetTransform(sitk.Transform())\n",
    "    resample.SetDefaultPixelValue(input_volume.GetPixelIDValue())\n",
    "\n",
    "    return resample.Execute(input_volume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resize_image(input_volume):\n",
    "    # Resize images to fixed spatial resolution in pixels\n",
    "    num_axial_slices = int(input_volume.GetSize()[-1])\n",
    "    output_size = [320, 320, num_axial_slices]\n",
    "    scale = np.divide(input_volume.GetSize(), output_size)\n",
    "    spacing = np.multiply(input_image.GetSpacing(), scale)\n",
    "    transform = sitk.AffineTransform(3)\n",
    "    resized_volume = sitk.Resample(input_volume, output_size, transform, sitk.sitkLinear, input_volume.GetOrigin(),\n",
    "                                  spacing, \n",
    "    input_volume.GetDirection())\n",
    "    return resized_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_slices(image_volume):\n",
    "    image_array = sitk.GetArrayFromImage(image_volume)\n",
    "    image_slices_array = image_array[int(np.shape(image_array)[0]):int(np.shape(image_array)[0]),:,:]\n",
    "    return image_slices_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_slice = sitk.GetImageFromArray(image_slices_array)\n",
    "sitk.WriteImage(image_slice, 'image_slice.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############DATA_SPLIT & TRANSFORM CODE: START#####################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import numpy as np\n",
    "train_transforms = transforms.Compose([\n",
    "                            transforms.RandomHorizontalFlip(p=0.1),\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "                                    transforms.Resize(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "# adapted from ptrblck post\n",
    "# custom dataset class in order to apply different transforms to different sets of data\n",
    "class LoadCustomDataSet(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            x = self.transform(img_dataset[index][0])\n",
    "        else:\n",
    "            x = img_dataset[index][0]\n",
    "        y = img_dataset[index][1]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(img_dataset)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# path to the directory which contains data folders belonging to various classes\n",
    "data_dir =  ''\n",
    "img_dataset = datasets.ImageFolder(data_dir)\n",
    "targets = img_dataset.targets\n",
    "# num_classes would be useful later while fine-tuning the pre-trained network\n",
    "num_classes = len(img_dataset.classes)\n",
    "\n",
    "# employing stratified train_test_split to make sure train and validation set have balanced data.\n",
    "train_id, valid_id= train_test_split(\n",
    "np.arange(len(targets)),\n",
    "test_size=0.2,\n",
    "shuffle=True,\n",
    "stratify=targets)\n",
    "\n",
    "train_set = LoadCustomDataSet(img_dataset, train_transforms)\n",
    "val_set = LoadCustomDataSet(img_dataset, val_transforms)\n",
    "\n",
    "train_data = Subset(train_set, indices=train_id)\n",
    "val_data = Subset(val_set, indices=valid_id)\n",
    "\n",
    "train_data_len = (len(train_id))\n",
    "val_data_len = (len(valid_id))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
    "##############DATA_SPLIT & TRANSFORM CODE: END#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "# path of the directory where the data is located\n",
    "data_dir = ''\n",
    "\n",
    "data_transforms = {\n",
    "# folder_name is the name of the root directory: assuming the data has not been split yet!\n",
    "    'folder_name': transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_dataset = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['folder_name']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_dataset[x], batch_size=20,\n",
    "                                             shuffle=False, num_workers=4)\n",
    "              for x in ['folder_name']}\n",
    "\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "total_samples = 0.\n",
    "# batch_samples are the samples being used at one point in time\n",
    "# data.size(1) is the number of channels in the data\n",
    "for input_images,_ in dataloaders['folder_name']:\n",
    "    batch_samples = input_images.size(0)\n",
    "    # Here, view is used to reshape the data and flatten the last two dimensions (320X320) size of images used in earlier code \n",
    "    input_images = input_images.view(batch_samples, input_images.size(1), -1)\n",
    "    mean += input_images.mean(2).sum(0)\n",
    "    std += input_images.std(2).sum(0)\n",
    "    total_samples += batch_samples\n",
    "    \n",
    "# calculation of final mean and standard deviation values \n",
    "mean /= total_samples\n",
    "std /= total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import time\n",
    "\n",
    "# use cuda if GPU support is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=2):\n",
    "    since = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0.0\n",
    "\n",
    "                i = 0\n",
    "                # Iterate over data.\n",
    "                for inputs,labels in (train_loader):\n",
    "                    # inputs, labels = data\n",
    "                    # print(data)\n",
    "                    # print(labels)\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                    # loss and accuracy\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += (torch.sum(preds == labels.data))\n",
    "                    # print(\"running corrects \",running_corrects)\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss = running_loss/train_data_len\n",
    "                # print(running_corrects)\n",
    "                train_acc = running_corrects/train_data_len\n",
    "\n",
    "\n",
    "                print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(\n",
    "                    train_loss, train_acc))\n",
    "\n",
    "            else:\n",
    "                # choose eval() mode for validation phase\n",
    "                val_acc, val_loss = eval_model(model, val_loader)\n",
    "                print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(\n",
    "                    val_loss, val_acc))\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    # print('Best test Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += float(torch.sum(preds == labels.data))\n",
    "    val_loss = running_loss / val_data_len\n",
    "    val_acc = running_corrects / val_data_len\n",
    "\n",
    "    return val_acc, val_loss\n",
    "\n",
    "\n",
    "# finetuning the model\n",
    "num_epochs = 25\n",
    "transfer_learn_network = models.resnet34(pretrained=True)\n",
    "num_ftrs = transfer_learn_network.fc.in_features\n",
    "\n",
    "\n",
    "transfer_learn_network.fc = nn.Linear(num_ftrs,len(classes))\n",
    "transfer_learn_network = transfer_learn_network.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(transfer_learn_network.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "policy = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# model training and evaluation\n",
    "train_model(transfer_learn_network, criterion, optimizer, policy,\n",
    "            num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finetuning the model\n",
    "model_ft = models.densenet121(pretrained=True)\n",
    "num_ftrs = model_ft.classifier.in_features\n",
    "\n",
    "model_ft.classifier = nn.Linear(num_ftrs, len(classes))\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# training and evaluating the model\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=2):\n",
    "    since = time.time()\n",
    "    conf_matrix_data = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                label_list = torch.zeros(train_data_len)\n",
    "                prediction_list = torch.zeros(train_data_len)\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0.0\n",
    "\n",
    "                i = 0\n",
    "                # Iterate over data.\n",
    "                for inputs,labels in (train_loader):\n",
    "                   \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            label_list[i:i + (len(labels))] = labels\n",
    "                            prediction_list[i:i + (len(labels))] = preds\n",
    "                            i += len(labels)\n",
    "                            package_label = zip(label_list, prediction_list)\n",
    "                            conf_matrix_data.append(package_label)\n",
    "                  \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += (torch.sum(preds == labels.data))\n",
    " \n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss = running_loss/train_data_len\n",
    "\n",
    "                train_acc = running_corrects/train_data_len\n",
    "\n",
    "\n",
    "                print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(\n",
    "                    train_loss, train_acc))\n",
    "\n",
    "            else:\n",
    "                # choose eval() mode for validation phase\n",
    "                val_acc, val_loss = eval_model(model, val_loader)\n",
    "                print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(\n",
    "                    val_loss, val_acc))\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    return conf_matrix_data\n",
    "    # print('Best test Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += float(torch.sum(preds == labels.data))\n",
    "    val_loss = running_loss / val_data_len\n",
    "    val_acc = running_corrects / val_data_len\n",
    "\n",
    "    return val_acc, val_loss\n",
    "\n",
    "\n",
    "# finetuning the model\n",
    "# set the number of epochs\n",
    "num_epochs = 25\n",
    "transfer_learn_network = models.resnet34(pretrained=True)\n",
    "num_ftrs = transfer_learn_network.fc.in_features\n",
    "\n",
    "\n",
    "transfer_learn_network.fc = nn.Linear(num_ftrs,len(classes))\n",
    "transfer_learn_network = transfer_learn_network.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(transfer_learn_network.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "policy = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# model training and evaluation\n",
    "conf_matrix_data = train_model(transfer_learn_network, criterion, optimizer, policy,\n",
    "            num_epochs=num_epochs)\n",
    "\n",
    "############################CONFUSION MATRIX CODE: START####################################################################\n",
    "confusion_matrix = torch.zeros(2,2)\n",
    "for i in range(len(conf_matrix_data)):\n",
    "    confusion_matrix = torch.zeros(2, 2)\n",
    "    for targ,pred in conf_matrix_data[i]:\n",
    "        predict_temp = pred.type(torch.LongTensor)\n",
    "        target_temp = targ.type(torch.LongTensor)\n",
    "        confusion_matrix[target_temp, predict_temp] += 1\n",
    "\n",
    "display_conf_matrix = []\n",
    "TP = confusion_matrix.diag()\n",
    "for class_num in range(len(classes)):\n",
    "    id = torch.ones(2).byte()\n",
    "    id[class_num] = 0\n",
    "    # all non-class samples classified as non-class\n",
    "    TN = confusion_matrix[\n",
    "        id.nonzero()[:,\n",
    "        None], id.nonzero()].sum()\n",
    "    FP = confusion_matrix[id, class_num].sum()\n",
    "    FN = confusion_matrix[class_num, id].sum()\n",
    "\n",
    "    Specificity = TN/(TN+FP)\n",
    "    Sensitivity = TP[class_num]/(TP[class_num] + FN)\n",
    "\n",
    "    display_conf_matrix.append([[int(TP[class_num]), int(FN)], [int(FP), int(TN)]])\n",
    "    print('Specificity of {} Class , {}'.format(\n",
    "        class_num,Specificity))\n",
    "    print('Sensitivity of {} Class , {}'.format(\n",
    "        class_num, Sensitivity))\n",
    "    print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(\n",
    "        class_num, TP[class_num], TN, FP, FN))\n",
    "\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display class-wise confusion matrix\n",
    "for count in range(len(classes)):\n",
    "    df_cm = pd.DataFrame(display_conf_matrix[count], index = ['Class '+str(count),'non-Class'],\n",
    "                  columns = ['Class '+str(count),'non-Class'])\n",
    "    title_str = \"Confusion matrix for class \" + str(count)\n",
    "\n",
    "    plt.figure(figsize = (10,7))\n",
    "\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.suptitle(title_str)\n",
    "    plt.ylabel('Actual Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.show()\n",
    "############################CONFUSION MATRIX CODE: END####################################################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
